{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from enum import Enum\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data \n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example = namedtuple(\"Example\", \"word pos head label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(fn):\n",
    "    examples = []\n",
    "    ex = Example([],[],[],[])\n",
    "    lines = map(lambda x: x.strip().split(), open(fn).readlines())\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            examples.append(ex)\n",
    "            ex = Example([],[],[],[])\n",
    "            continue\n",
    "        ex.word.append(line[1])\n",
    "        ex.pos.append(line[3])\n",
    "        ex.head.append(int(line[6])-1)\n",
    "        ex.label.append(line[7])\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_examples(\"data/train.conll\")\n",
    "dev_set = load_examples(\"data/dev.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_unique(examples, idx):\n",
    "    result = []\n",
    "    for ex in examples:\n",
    "        result.extend(ex[idx])\n",
    "    return sorted(list(set(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(object):\n",
    "    def __init__(self, tokens, dim, filename=None):\n",
    "        self.tokens = sorted(tokens)\n",
    "        self.token2id = {token.lower() : n for (n, token) in enumerate(tokens)}\n",
    "        self.embedding_dim = dim\n",
    "        self.NULL = \"<NULL>\"\n",
    "        self.UNK = \"<UNK>\"\n",
    "        for t in [self.NULL, self.UNK]:\n",
    "            self.token2id[t] = len(self.tokens)\n",
    "            self.tokens.append(t)\n",
    "        self.NULL_ID = self.get_id(self.NULL) \n",
    "        self.embed = nn.Embedding(len(self.tokens), self.embedding_dim)\n",
    "        self.embed.weight.data.copy_(self.get_initial(filename))\n",
    "        \n",
    "    def get_initial(self, filename):\n",
    "        matrix = (2*np.random.rand(len(self.tokens), self.embedding_dim)-1)/100 # -0.01<->0.01\n",
    "        if filename:\n",
    "            loaded = 0\n",
    "            lines = open(filename, \"rt\").readlines()\n",
    "            for line in lines:\n",
    "                data = line.split()\n",
    "                word = data[0].lower()\n",
    "                if word in self.token2id:\n",
    "                    loaded += 1\n",
    "                    idx = self.token2id[word]\n",
    "                    vec = list(map(float, data[1:]))\n",
    "                    matrix[idx, :] = vec\n",
    "            print(\"Loaded \", loaded, \" pre-trained word vectors\")\n",
    "        return torch.from_numpy(matrix)\n",
    "    \n",
    "    \n",
    "    def get_id(self, token):\n",
    "        if isinstance(token, int):\n",
    "            return token\n",
    "        if token in self.token2id:\n",
    "            return self.token2id[token]\n",
    "        else:\n",
    "            return self.token2id[self.UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  27518  pre-trained word vectors\n"
     ]
    }
   ],
   "source": [
    "word_embedding = Embedding(collect_unique(train_set, 0), 50, \"data/en-cw.txt\")\n",
    "pos_embedding = Embedding(collect_unique(train_set, 1), 50)\n",
    "label_embedding = Embedding(collect_unique(train_set, 3), 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionType(Enum):\n",
    "    SHIFT = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "Transition = namedtuple(\"Transition\", \"kind label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(3, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, word_embeddings, label_embeddings, pos_embeddings, D_in, hidden, D_out):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = word_embeddings.embed  \n",
    "        self.label_embeddings = label_embeddings.embed\n",
    "        self.pos_embeddings = pos_embeddings.embed\n",
    "        self.fc1 = nn.Linear(D_in, hidden)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight.data)\n",
    "        self.fc1_dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(hidden, D_out)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight.data)\n",
    "    \n",
    "    def forward(self, word_ids, pos_ids, label_ids):\n",
    "        W = self.word_embeddings(word_ids)\n",
    "        P = self.pos_embeddings(pos_ids)\n",
    "        L = self.label_embeddings(label_ids)\n",
    "        #print(W.shape, P.shape, L.shape)\n",
    "        W = torch.reshape(W, (W.shape[0], np.prod(W.shape[1:])))\n",
    "        #W = W.view((1, -1))\n",
    "        P = torch.reshape(P, (P.shape[0], np.prod(P.shape[1:])))\n",
    "        L = torch.reshape(L, (L.shape[0], np.prod(L.shape[1:])))\n",
    "        #print(W.shape, P.shape, L.shape)\n",
    "        combined = torch.cat((W, P, L), 1)\n",
    "        relu = self.fc1(combined).clamp(min=0)\n",
    "        dropped = self.fc1_dropout(relu)\n",
    "        return self.fc2(dropped)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arc = namedtuple(\"Arc\", \"head word label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse(object):\n",
    "    def __init__(self, sentence, W_NULL=\"<NULL>\", P_NULL=\"<NULL>\", L_NULL=\"<NULL>\"):\n",
    "        self.sentence = sentence\n",
    "        N = len(sentence.word)\n",
    "        self.buf = [N - i - 1 for i in range(N)]\n",
    "        self.stack = []\n",
    "        self.arcs = []\n",
    "        self.W_NULL = W_NULL\n",
    "        self.P_NULL = P_NULL\n",
    "        self.L_NULL = L_NULL\n",
    "        \n",
    "    \n",
    "    def transition(self, transition, set_label=False):\n",
    "        if transition.kind == TransitionType.SHIFT:\n",
    "            self.stack.append(self.buf.pop())\n",
    "        elif transition.kind == TransitionType.LEFT:\n",
    "            if set_label:\n",
    "                self.sentence.label[self.stack[-2]] = transition.label\n",
    "                self.sentence.head[self.stack[-2]] = self.stack[-1]\n",
    "            self.arcs.append((self.stack[-1], self.stack[-2], transition.label))\n",
    "            self.stack.pop(-2)\n",
    "        else: # RIGHT\n",
    "            if set_label:\n",
    "                self.sentence.label[self.stack[-1]] = transition.label\n",
    "                self.sentence.head[self.stack[-1]] = self.stack[-2]\n",
    "            self.arcs.append((self.stack[-2], self.stack[-1], transition.label))\n",
    "            self.stack.pop()\n",
    "    \n",
    "    def get_oracle(self):\n",
    "        if len(self.stack) < 2:\n",
    "            if len(self.buf) > 0:\n",
    "                return Transition(TransitionType.SHIFT, 0)\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        i0 = self.stack[-1]\n",
    "        i1 = self.stack[-2]\n",
    "        head0 = self.sentence.head[i0]\n",
    "        head1 = self.sentence.head[i1]\n",
    "        label0 = self.sentence.label[i0]\n",
    "        label1 = self.sentence.label[i1]\n",
    "\n",
    "        if (i1 >= 0) and (head1 == i0):\n",
    "            return Transition(TransitionType.LEFT, label1)\n",
    "        elif (i1 >= 0) and (head0 == i1) and \\\n",
    "             (not any([x for x in self.buf if self.sentence.head[x] == i0])): # don't remove i0 if we still need it\n",
    "            return Transition(TransitionType.RIGHT, label0)\n",
    "        elif len(self.buf) > 0:\n",
    "            return Transition(TransitionType.SHIFT, 0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_cur_features(self):\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in self.arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in self.arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "        \n",
    "        \n",
    "        w_features = []\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        \n",
    "        def add(source_idx, source, target, i, null):\n",
    "            if i >= len(source_idx):\n",
    "                target.append(null)\n",
    "            else:\n",
    "                target.append(source[source_idx[-(i+1)]])\n",
    "        for i in range(3):\n",
    "            add(self.buf, self.sentence.word, w_features, i, self.W_NULL)\n",
    "            add(self.stack, self.sentence.word, w_features, i, self.W_NULL)\n",
    "            add(self.buf, self.sentence.pos, p_features, i, self.P_NULL)\n",
    "            add(self.stack, self.sentence.pos, p_features, i, self.P_NULL)\n",
    "        \n",
    "        def add2(target, source, arr, idx, null):\n",
    "            if len(arr) > idx:\n",
    "                target.append(source[arr[idx]])\n",
    "            else:\n",
    "                target.append(null)\n",
    "        \n",
    "        for i in range(2):\n",
    "            if i < len(self.stack):\n",
    "                k = self.stack[-i-1]\n",
    "                lc = get_lc(k)\n",
    "                rc = get_rc(k)\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                for target, source, null in [(w_features, self.sentence.word, self.W_NULL),\n",
    "                                       (p_features, self.sentence.pos, self.P_NULL),\n",
    "                                       (l_features, self.sentence.label, self.L_NULL)]:\n",
    "                    for arr, idx in [(lc, 0), (rc, 0), (lc, 1), (rc, 1), (llc, 0), (rrc, 0)]:\n",
    "                        add2(target, source, arr, idx, null)\n",
    "            else:\n",
    "                w_features += [self.W_NULL] * 6\n",
    "                p_features += [self.P_NULL] * 6\n",
    "                l_features += [self.L_NULL] * 6\n",
    "        \n",
    "        # n_features = 48\n",
    "        return w_features, p_features, l_features\n",
    "    \n",
    "    def get_instances(self):\n",
    "        result = []\n",
    "        transition = self.get_oracle()\n",
    "        while transition:\n",
    "            result.append((self.get_cur_features(), transition))\n",
    "            self.transition(transition)\n",
    "            transition = self.get_oracle()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, word_embeddings, label_embeddings, pos_embeddings):\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.label_embeddings = label_embeddings\n",
    "        self.pos_embeddings = pos_embeddings\n",
    "        self.transitions = [Transition(TransitionType.SHIFT, 0)]\n",
    "        for i in range(len(label_embeddings.tokens) - 2): # to cut off <UNK> and <NULL>:\n",
    "            self.transitions.append(Transition(TransitionType.LEFT, i))\n",
    "            self.transitions.append(Transition(TransitionType.RIGHT, i))\n",
    "        \n",
    "        self.transition2id = {tran:i for (i, tran) in enumerate(self.transitions)} \n",
    "        self.model = Predictor(word_embeddings, label_embeddings, pos_embeddings, 48*50, 200, len(self.transitions))\n",
    "    \n",
    "    def vectorize(self, sentence):\n",
    "        return Example([self.word_embeddings.get_id(t) for t in sentence.word],\n",
    "                       [self.pos_embeddings.get_id(t) for t in sentence.pos],\n",
    "                       sentence.head,\n",
    "                       [self.label_embeddings.get_id(t) for t in sentence.label])\n",
    "    \n",
    "    \n",
    "    def create_dataset(self, examples):\n",
    "        samples = []\n",
    "        for ex in tqdm.tqdm(examples):\n",
    "            parse = Parse(self.vectorize(ex), self.word_embeddings.NULL_ID, \n",
    "                                              self.pos_embeddings.NULL_ID, \n",
    "                                              self.label_embeddings.NULL_ID)\n",
    "            for x, y in parse.get_instances():\n",
    "                samples.append({\"w\": x[0], \"p\" : x[1], \"l\" : x[2], \"label\" : self.transition2id[y]})\n",
    "        \n",
    "        dataset = ParseDataset(samples)\n",
    "        return dataset\n",
    "    \n",
    "    def train(self, dataset, batch_size=10000, epochs=1, lr=1e-2):\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optim = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "        i = 0\n",
    "        #print(list(map(lambda x: x.shape, self.model.parameters())))\n",
    "        for _ in range(epochs):\n",
    "            for samples in tqdm.tqdm(dataloader):\n",
    "                i += 1\n",
    "                #print(samples)\n",
    "                optim.zero_grad()\n",
    "                w = torch.stack(samples[\"w\"], 1)\n",
    "                p = torch.stack(samples[\"p\"], 1)\n",
    "                l = torch.stack(samples[\"l\"], 1)\n",
    "                #print(w.shape, p.shape, l.shape)\n",
    "                pred = self.model(w, p, l)\n",
    "                output = loss(pred, samples[\"label\"])\n",
    "                #if i % 50 == 1:\n",
    "                #    print(\"loss: \", output.data)\n",
    "                output.backward()\n",
    "                optim.step()\n",
    "\n",
    "                \n",
    "    def parse_sentence(self, sentence, score=True):\n",
    "        vectorized = Example([self.word_embeddings.get_id(t) for t in sentence.word],\n",
    "                           [self.pos_embeddings.get_id(t) for t in sentence.pos],\n",
    "                           [-1 for t in sentence.word],\n",
    "                           [self.label_embeddings.NULL_ID for t in sentence.word])\n",
    "        parse = Parse(vectorized, self.word_embeddings.NULL_ID, \n",
    "                                          self.pos_embeddings.NULL_ID, \n",
    "                                          self.label_embeddings.NULL_ID)\n",
    "        SHIFT = Transition(TransitionType.SHIFT, 0)\n",
    "        SHIFT_ID = self.transition2id[SHIFT]\n",
    "        while len(parse.stack) >= 2 or len(parse.buf) > 0: \n",
    "            w, p, l = parse.get_cur_features()\n",
    "            w = torch.unsqueeze(torch.LongTensor(w), 0)\n",
    "            p = torch.unsqueeze(torch.LongTensor(p), 0)\n",
    "            l = torch.unsqueeze(torch.LongTensor(l), 0)\n",
    "            if len(parse.stack) < 2 and len(parse.buf) > 0:\n",
    "                transition = SHIFT\n",
    "            else:\n",
    "                pred = self.model(w, p, l).clone().detach().numpy()\n",
    "                if len(parse.buf) == 0:\n",
    "                    pred[SHIFT_ID] = -np.inf\n",
    "                transition = self.transitions[np.argmax(pred)]\n",
    "                if len(parse.buf) == 0 and transition == SHIFT:\n",
    "                    transition = Transition(TransitionType.RIGHT, 0)\n",
    "\n",
    "            #print(\"Transition: \", transition)\n",
    "            parse.transition(transition, True)\n",
    "            \n",
    "        if score:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for p_h, c_h in zip(parse.sentence.head, sentence.head):\n",
    "                if c_h == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    total += 1\n",
    "                    correct += int(p_h == c_h)\n",
    "            return total, correct\n",
    "        \n",
    "    def compute_UAS(self, dataset):\n",
    "        all_tokens = 0\n",
    "        correct_tokens = 0\n",
    "        for ex in tqdm.tqdm(dataset):\n",
    "            a,c = self.parse_sentence(ex)\n",
    "            all_tokens += a\n",
    "            correct_tokens += c\n",
    "        return correct_tokens/all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(word_embedding, label_embedding, pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39832"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39832/39832 [00:55<00:00, 718.91it/s]\n",
      "100%|██████████| 186/186 [05:01<00:00,  1.62s/it]\n",
      "100%|██████████| 1700/1700 [00:43<00:00, 39.25it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.6103808209907072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [04:51<00:00,  1.57s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 59.21it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.7254080224900434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:07<00:00,  1.66s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 58.98it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.7554468074029727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:14<00:00,  1.69s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 60.25it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.7748392638675586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:22<00:00,  1.74s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 59.44it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.7881666970351667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:18<00:00,  1.71s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 59.85it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.792695941900721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [04:59<00:00,  1.61s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 59.27it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.8006351354868938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:14<00:00,  1.69s/it]\n",
      "100%|██████████| 1700/1700 [00:27<00:00, 60.91it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.8036806622068355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:20<00:00,  1.73s/it]\n",
      "100%|██████████| 1700/1700 [00:27<00:00, 61.93it/s]\n",
      "  0%|          | 0/186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.8075851836426582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [05:15<00:00,  1.70s/it]\n",
      "100%|██████████| 1700/1700 [00:28<00:00, 60.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS= 0.8127651820808496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_UAS = []\n",
    "train_dataset = parser.create_dataset(train_set)\n",
    "for _ in range(10):\n",
    "    parser.train(train_dataset, lr=5e-4)\n",
    "    UAS = parser.compute_UAS(dev_set)\n",
    "    print(\"UAS=\", UAS)\n",
    "    dev_UAS.append(UAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5962792315048249, 0.673041815178649]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev_UAS \n",
    "# no dropout - [0.5962792315048249, 0.673041815178649]\n",
    "# with dropout - [0.5324697904894375, 0.5870642441102321]\n",
    "# On full data: 0.69, 0.73\n",
    "# updated: 0.66, 0.73, 0.75, 0.76, 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5324697904894375, 0.5870642441102321]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_sentence(dev_set[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence2(self, sentence, score=True):\n",
    "    vectorized = Example([self.word_embeddings.get_id(t) for t in sentence.word],\n",
    "                       [self.pos_embeddings.get_id(t) for t in sentence.pos],\n",
    "                       [-1 for t in sentence.word],\n",
    "                       [self.label_embeddings.NULL_ID for t in sentence.word])\n",
    "    parse = Parse(vectorized, self.word_embeddings.NULL_ID, \n",
    "                                      self.pos_embeddings.NULL_ID, \n",
    "                                      self.label_embeddings.NULL_ID)\n",
    "    SHIFT = Transition(TransitionType.SHIFT, 0)\n",
    "    SHIFT_ID = self.transition2id[SHIFT]\n",
    "    while len(parse.stack) >= 2 or len(parse.buf) > 0: \n",
    "        w, p, l = parse.get_cur_features()\n",
    "        w = torch.unsqueeze(torch.LongTensor(w), 0)\n",
    "        p = torch.unsqueeze(torch.LongTensor(p), 0)\n",
    "        l = torch.unsqueeze(torch.LongTensor(l), 0)\n",
    "        if len(parse.stack) < 2 and len(parse.buf) > 0:\n",
    "            transition = SHIFT\n",
    "        else:\n",
    "            pred = self.model(w, p, l).clone().detach().numpy()\n",
    "            if len(parse.buf) == 0:\n",
    "                pred[SHIFT_ID] = -np.inf\n",
    "            transition = self.transitions[np.argmax(pred)]\n",
    "            if len(parse.buf) == 0 and transition == SHIFT:\n",
    "                transition = Transition(TransitionType.RIGHT, 0)\n",
    "\n",
    "        #print(\"Transition: \", transition)\n",
    "        parse.transition(transition, True)\n",
    "    print(parse.sentence.head)\n",
    "    if score:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for p_h, c_h in zip(parse.sentence.head, sentence.head):\n",
    "            if c_h == -1:\n",
    "                continue\n",
    "            else:\n",
    "                total += 1\n",
    "                correct += int(p_h == c_h)\n",
    "        return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 4, 4, -1, 6, 4, 8, 6, 10, 8, 13, 13, 10, 16, 16, 13, 19, 19, 4, 21, 19, 23, 21, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_sentence2(parser, dev_set[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(word=['PaineWebber', 'Inc.', 'filmed', 'a', 'new', 'television', 'commercial', 'at', '4', 'p.m.', 'EDT', 'yesterday', 'and', 'had', 'it', 'on', 'the', 'air', 'by', 'last', 'night', '.'], pos=['PROPN', 'PROPN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'NUM', 'NOUN', 'PROPN', 'NOUN', 'CONJ', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT'], head=[1, 2, -1, 6, 6, 6, 2, 10, 10, 10, 2, 2, 2, 2, 13, 17, 17, 13, 20, 20, 13, 2], label=['compound', 'nsubj', 'root', 'det', 'amod', 'compound', 'dobj', 'case', 'nummod', 'compound', 'nmod', 'nmod:tmod', 'cc', 'conj', 'dobj', 'case', 'det', 'nmod', 'case', 'amod', 'nmod', 'punct'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
